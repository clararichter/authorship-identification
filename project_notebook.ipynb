{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dating Documents\n",
    "\n",
    "1) Prediction you hope to make\n",
    "\n",
    "Given a short text in English, we wish to perform a classification in predicting the year (or the decade in which) it was written. The writing style of a written work from, say the 1800s, is certainly very different from that of a modern text, and we hope to build a model that can recognize this difference in a consistent way. The related, but different, problem of \"author identification\" has been tackled before (e.g. see [1] and [2]). \n",
    "\n",
    "2) Data sources\n",
    "- http://www.gutenberg.org/\n",
    "- https://books.google.com/ngrams\n",
    "- https://googlebooks.byu.edu/x.asp (more advanced word tool)\n",
    "\n",
    "3) Data cleaning plan - what will you need to do to make the data usable? Include tables showing where data issues are.\n",
    "\n",
    "Given a written work that can be accessed from gutenberg.org, we wish to obtain the following data:\n",
    "- a dataset which captures word frequency. NLTK makes this process very simple. Using the \"FreqDist\" class, we can produce a list with all words and the number of times they appear in the form of a list of tuples: [(<word>, <word count>), ...].\n",
    "- other relevant metrics??? \n",
    "\n",
    "Concern 1: do we analyze the entirety of the written work, or smaller paragraphs of it? If the paragraphs are too short, this potentially diminishes the informativeness of word frequency; if a paragraph consists of 500 words and 2/3 of these are generic words such as \"and\", \"if\" and \"but\", then perhaps too little room is left for epoch-characteristic words. \n",
    "\n",
    "Concern 2: Google n-grams \"charts the frequencies of any set of comma-delimited search strings using a yearly count of n-grams found in sources printed between 1500 and 2008 in Google's text corpora\". However, the frequency for any given year should depend on <i> the number of works that were published in that given year </i>. It mot likely the case that fewer works from the 1500s are incorporated in Google's text corpora than those of the 2000's. So the question is whether Google n-grams really capture the <i> relative commonness </i> of a word between time periods?\n",
    "   \n",
    " \n",
    " \n",
    "\n",
    "4) Data exploration/visualization\n",
    "\n",
    "\n",
    "[1] https://web.stanford.edu/class/cs224n/reports/2760185.pdf\n",
    "[2] https://brage.bibsys.no/xmlui/bitstream/handle/11250/2353615/12344_FULLTEXT.pdf?sequence=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
